data:
    modality: "pose"
    train_pipeline:
        dataset:
            _target_: openhands.datasets.isolated.bhosporus22k.Bhosporus22kDataset
            class_mappings_file_path: "/datadrive/BosphorusSign22k/BosphorusSign22k.csv"
            root_dir: "/datadrive/BosphorusSign22k/PKL_POSES"
            split_file: "/datadrive/BosphorusSign22k/BosphorusSign22k.csv"
            splits: "train"
            modality: "pose"

        transforms:
            - PoseSelect:
                preset: mediapipe_holistic_minimal_27
            # - PoseTemporalSubsample:
            #       num_frames: 32
            # - RandomMove:
            - CenterAndScaleNormalize:
                reference_points_preset: shoulder_mediapipe_holistic_minimal_27
                scale_factor: 1
            - ShearTransform:
                shear_std: 0.1
            - RotatationTransform:
                rotation_std: 0.1
            # - ScaleTransform:
            #     scale_std: 0.2

        dataloader:
            _target_: torch.utils.data.DataLoader
            batch_size: 64
            shuffle: true
            num_workers: 6
            pin_memory: true
            drop_last: true

    valid_pipeline:
        dataset:
            _target_: openhands.datasets.isolated.bhosporus22k.Bhosporus22kDataset
            class_mappings_file_path: "/datadrive/BosphorusSign22k/BosphorusSign22k.csv"
            root_dir: "/datadrive/BosphorusSign22k/PKL_POSES"
            split_file: "/datadrive/BosphorusSign22k/BosphorusSign22k.csv"
            splits: "val"
            modality: "pose"

        transforms:
            - PoseSelect:
                preset: mediapipe_holistic_minimal_27
            # - PoseTemporalSubsample:
            #       num_frames: 32
            - CenterAndScaleNormalize:
                reference_points_preset: shoulder_mediapipe_holistic_minimal_27
                scale_factor: 1

        dataloader:
            _target_: torch.utils.data.DataLoader
            batch_size: 32
            shuffle: false
            num_workers: 6
            pin_memory: true
            drop_last: false

model:
    encoder:
        type: pose-flattener
        params:
            num_points: 27
    decoder:
        type: bert
        params:
            max_position_embeddings: 256
            layer_norm_eps: 1e-12
            hidden_dropout_prob: 0.1
            hidden_size: 128
            num_attention_heads: 8
            num_hidden_layers: 5
            cls_token: true

optim:
    loss: 'CrossEntropyLoss'
    optimizer:
        name: Adam
        params:
            lr: 1e-4

    scheduler:
        name: CosineAnnealingLR
        params:
            last_epoch: -1
            T_max: 10

trainer:
    gpus: 1
    max_epochs: 1500

exp_manager:
    create_tensorboard_logger: true
    create_wandb_logger: true
    wandb_logger_kwargs:
        name: "manideepladi"
        project: "Bhosphorus22k"

    create_checkpoint_callback: true
    checkpoint_callback_params:
        monitor: "val_acc"
        mode: "max"
        save_top_k: 3
        dirpath: "experiments/bhosphrous/pose_transformer/"

    early_stopping_callback: true
    early_stopping_params:
        monitor: "val_acc"
        patience: 100
        verbose: true
        mode: "max"
